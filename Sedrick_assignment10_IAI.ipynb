{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sedkameni/IntroAI_Willis/blob/main/Sedrick_assignment10_IAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D08tLfk7A7pa"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLhRoOZi-3Ex"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "IMDB Sentiment Analysis: Traditional ML & BERT Implementation\n",
        "Dataset: IMDB Movie Reviews (50,000 reviews)\n",
        "Author: Sedrick\n",
        "Date: November 2025\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Comprehensive text preprocessing\n",
        "- Feature engineering (BoW, TF-IDF, Word Embeddings)\n",
        "- Traditional ML models (Logistic Regression, SVM)\n",
        "- BERT fine-tuning for sentiment analysis\n",
        "- Model evaluation and comparison\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "from collections import Counter\n",
        "\n",
        "# Text preprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Feature engineering\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import spacy\n",
        "\n",
        "# Dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
        ")\n",
        "\n",
        "# Deep Learning (BERT)\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
        ")\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOcqA_NdBpXM"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2. LOAD AND EXPLORE DATASET\n",
        "# ============================================================================\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Replace with your uploaded filename\n",
        "zip_path = \"IMDB Dataset.csv.zip\"\n",
        "\n",
        "# Extract the contents\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "# Find out what files were extracted\n",
        "import os\n",
        "os.listdir(\".\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET LOADING & EXPLORATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Number of Reviews: {len(df)}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Information:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nSentiment Distribution:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Visualize sentiment distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "df['sentiment'].value_counts().plot(kind='bar', color=['salmon', 'skyblue'])\n",
        "plt.title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Review length statistics\n",
        "df['review_length'] = df['review'].apply(len)\n",
        "df['word_count'] = df['review'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "print(\"\\nReview Length Statistics:\")\n",
        "print(df[['review_length', 'word_count']].describe())\n",
        "\n",
        "# Visualize review lengths by sentiment\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].hist(df[df['sentiment']=='positive']['word_count'], bins=50, alpha=0.7, label='Positive', color='green')\n",
        "axes[0].hist(df[df['sentiment']=='negative']['word_count'], bins=50, alpha=0.7, label='Negative', color='red')\n",
        "axes[0].set_xlabel('Word Count')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Word Count Distribution by Sentiment', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].set_xlim(0, 1000)\n",
        "\n",
        "axes[1].boxplot([df[df['sentiment']=='positive']['word_count'],\n",
        "                 df[df['sentiment']=='negative']['word_count']],\n",
        "                labels=['Positive', 'Negative'])\n",
        "axes[1].set_ylabel('Word Count')\n",
        "axes[1].set_title('Word Count Box Plot by Sentiment', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv7xSzkXCvBA"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3. DATA PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values before handling: {df.isnull().sum().sum()}\")\n",
        "df = df.dropna()\n",
        "print(f\"Missing values after handling: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Encode labels\n",
        "print(\"\\nEncoding sentiment labels\")\n",
        "df['sentiment_encoded'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "print(\"Label encoding complete: positive=1, negative=0\")\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive text cleaning:\n",
        "    - Remove HTML tags\n",
        "    - Remove URLs\n",
        "    - Remove special characters\n",
        "    - Convert to lowercase\n",
        "    - Remove extra whitespace\n",
        "    \"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove special characters and digits (keep letters and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"\\nCleaning text data...\")\n",
        "df['review_cleaned'] = df['review'].apply(clean_text)\n",
        "\n",
        "print(\"\\nExample of cleaned text:\")\n",
        "print(f\"Original: {df['review'].iloc[0][:200]}...\")\n",
        "print(f\"Cleaned: {df['review_cleaned'].iloc[0][:200]}...\")\n",
        "\n",
        "# Advanced preprocessing with lemmatization and stopword removal\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Download punkt_tab resource\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Advanced preprocessing:\n",
        "    - Tokenization\n",
        "    - Stopword removal\n",
        "    - Lemmatization\n",
        "    \"\"\"\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens\n",
        "              if token not in stop_words and len(token) > 2]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"\\nApplying advanced preprocessing (this may take a few minutes)...\")\n",
        "# For demo purposes, process subset (full dataset takes time)\n",
        "df_sample = df.sample(n=5000, random_state=42)  # Use full df for production\n",
        "df_sample['review_processed'] = df_sample['review_cleaned'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\nExample of processed text:\")\n",
        "print(f\"Cleaned: {df_sample['review_cleaned'].iloc[0][:150]}...\")\n",
        "print(f\"Processed: {df_sample['review_processed'].iloc[0][:150]}...\")\n",
        "\n",
        "# For full implementation, use entire dataset\n",
        "df['review_processed'] = df['review_cleaned'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\n Preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70k97-xVG6Ws"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Split dataset FIRST (to avoid data leakage)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['review_processed'],\n",
        "    df['sentiment_encoded'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['sentiment_encoded']\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "print(f\"Class distribution in training: {y_train.value_counts()}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4.1 BAG OF WORDS (BoW)\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n--- BAG OF WORDS (BoW) ---\")\n",
        "\n",
        "bow_vectorizer = CountVectorizer(\n",
        "    max_features=5000,  # Limit vocabulary size\n",
        "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "    min_df=5  # Ignore terms appearing in < 5 documents\n",
        ")\n",
        "\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"BoW training shape: {X_train_bow.shape}\")\n",
        "print(f\"BoW test shape: {X_test_bow.shape}\")\n",
        "print(f\"Vocabulary size: {len(bow_vectorizer.vocabulary_)}\")\n",
        "\n",
        "# Most common words\n",
        "bow_freq = X_train_bow.sum(axis=0).A1\n",
        "bow_words = bow_vectorizer.get_feature_names_out()\n",
        "top_words_bow = sorted(zip(bow_words, bow_freq), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "print(\"\\nTop 20 words in BoW:\")\n",
        "for word, freq in top_words_bow[:10]:\n",
        "    print(f\"  {word}: {freq}\")\n",
        "\n",
        "# Visualize top words\n",
        "plt.figure(figsize=(12, 6))\n",
        "words, freqs = zip(*top_words_bow)\n",
        "plt.barh(words, freqs, color='steelblue')\n",
        "plt.xlabel('Frequency')\n",
        "plt.title('Top 20 Words in Bag of Words', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 4.2 TF-IDF\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n--- TF-IDF (Term Frequency - Inverse Document Frequency) ---\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5,\n",
        "    max_df=0.8,  # Ignore terms appearing in > 80% of documents\n",
        "    sublinear_tf=True  # Apply sublinear tf scaling\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"TF-IDF training shape: {X_train_tfidf.shape}\")\n",
        "print(f\"TF-IDF test shape: {X_test_tfidf.shape}\")\n",
        "\n",
        "# Top TF-IDF features\n",
        "tfidf_scores = X_train_tfidf.sum(axis=0).A1\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "top_words_tfidf = sorted(zip(tfidf_words, tfidf_scores), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "print(\"\\nTop 20 words by TF-IDF score:\")\n",
        "for word, score in top_words_tfidf[:10]:\n",
        "    print(f\"  {word}: {score:.2f}\")\n",
        "\n",
        "# Visualize top TF-IDF words\n",
        "plt.figure(figsize=(12, 6))\n",
        "words, scores = zip(*top_words_tfidf)\n",
        "plt.barh(words, scores, color='coral')\n",
        "plt.xlabel('TF-IDF Score')\n",
        "plt.title('Top 20 Words by TF-IDF Score', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 4.3 WORD EMBEDDINGS (Word2Vec)\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n--- WORD EMBEDDINGS (Word2Vec) ---\")\n",
        "\n",
        "# Tokenize for Word2Vec\n",
        "print(\"Training Word2Vec model...\")\n",
        "sentences = [text.split() for text in X_train]\n",
        "\n",
        "# Train Word2Vec\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,  # Embedding dimension\n",
        "    window=5,  # Context window\n",
        "    min_count=5,  # Ignore words with frequency < 5\n",
        "    workers=4,\n",
        "    sg=1  # Skip-gram (1) vs CBOW (0)\n",
        ")\n",
        "\n",
        "print(f\"Word2Vec vocabulary size: {len(w2v_model.wv)}\")\n",
        "print(f\"Embedding dimension: {w2v_model.vector_size}\")\n",
        "\n",
        "# Example: Similar words\n",
        "print(\"\\nWords most similar to 'good':\")\n",
        "try:\n",
        "    similar_words = w2v_model.wv.most_similar('good', topn=10)\n",
        "    for word, score in similar_words:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "except KeyError:\n",
        "    print(\"  'good' not in vocabulary\")\n",
        "\n",
        "# Document embeddings (average word vectors)\n",
        "def document_vector(text, model):\n",
        "    \"\"\"Average word vectors for document representation\"\"\"\n",
        "    words = text.split()\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "print(\"\\nCreating document embeddings...\")\n",
        "X_train_w2v = np.array([document_vector(text, w2v_model) for text in X_train])\n",
        "X_test_w2v = np.array([document_vector(text, w2v_model) for text in X_test])\n",
        "\n",
        "print(f\"Word2Vec training shape: {X_train_w2v.shape}\")\n",
        "print(f\"Word2Vec test shape: {X_test_w2v.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4.4 VISUALIZE WORD EMBEDDINGS\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n--- VISUALIZING WORD EMBEDDINGS ---\")\n",
        "\n",
        "# Select common words for visualization\n",
        "common_words = [word for word, freq in top_words_bow[:100] if word in w2v_model.wv][:50]\n",
        "word_vectors = np.array([w2v_model.wv[word] for word in common_words])\n",
        "\n",
        "print(f\"\\nVisualizing {len(common_words)} word embeddings...\")\n",
        "\n",
        "# t-SNE for 2D visualization\n",
        "print(\"Applying t-SNE dimensionality reduction...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=15, n_iter=1000)\n",
        "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], alpha=0.6, s=100, c='steelblue')\n",
        "\n",
        "for i, word in enumerate(common_words):\n",
        "    plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]),\n",
        "                xytext=(5, 2), textcoords='offset points', fontsize=9, alpha=0.7)\n",
        "\n",
        "plt.title('Word Embeddings Visualization (t-SNE)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# PCA visualization\n",
        "print(\"Applying PCA dimensionality reduction...\")\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "word_vectors_pca = pca.fit_transform(word_vectors)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(word_vectors_pca[:, 0], word_vectors_pca[:, 1], alpha=0.6, s=100, c='coral')\n",
        "\n",
        "for i, word in enumerate(common_words):\n",
        "    plt.annotate(word, xy=(word_vectors_pca[i, 0], word_vectors_pca[i, 1]),\n",
        "                xytext=(5, 2), textcoords='offset points', fontsize=9, alpha=0.7)\n",
        "\n",
        "plt.title('Word Embeddings Visualization (PCA)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Feature engineering complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9AOOyUlIPzy"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 5. TRADITIONAL ML MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRADITIONAL MACHINE LEARNING MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# -----------------------------\n",
        "# 5.1 LOGISTIC REGRESSION\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n--- LOGISTIC REGRESSION ---\")\n",
        "\n",
        "# Train on TF-IDF features\n",
        "print(\"\\nTraining Logistic Regression on TF-IDF features...\")\n",
        "\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    C=1.0,\n",
        "    solver='liblinear'\n",
        ")\n",
        "\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
        "y_proba_lr = lr_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "prec_lr = precision_score(y_test, y_pred_lr)\n",
        "rec_lr = recall_score(y_test, y_pred_lr)\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "roc_auc_lr = roc_auc_score(y_test, y_proba_lr)\n",
        "\n",
        "results['Logistic Regression'] = {\n",
        "    'accuracy': acc_lr,\n",
        "    'precision': prec_lr,\n",
        "    'recall': rec_lr,\n",
        "    'f1': f1_lr,\n",
        "    'roc_auc': roc_auc_lr,\n",
        "    'predictions': y_pred_lr,\n",
        "    'probabilities': y_proba_lr\n",
        "}\n",
        "\n",
        "print(f\"\\nLogistic Regression Results:\")\n",
        "print(f\"  Accuracy:  {acc_lr:.4f}\")\n",
        "print(f\"  Precision: {prec_lr:.4f}\")\n",
        "print(f\"  Recall:    {rec_lr:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_lr:.4f}\")\n",
        "print(f\"  ROC-AUC:   {roc_auc_lr:.4f}\")\n",
        "\n",
        "# Grid Search for hyperparameter tuning\n",
        "print(\"\\nPerforming Grid Search for Logistic Regression...\")\n",
        "\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid_lr = GridSearchCV(\n",
        "    LogisticRegression(max_iter=1000, random_state=42),\n",
        "    param_grid_lr,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_lr.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_lr.best_params_}\")\n",
        "print(f\"Best F1-score (CV): {grid_lr.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate best model\n",
        "y_pred_lr_best = grid_lr.predict(X_test_tfidf)\n",
        "y_proba_lr_best = grid_lr.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "print(f\"\\nOptimized Logistic Regression Test F1-Score: {f1_score(y_test, y_pred_lr_best):.4f}\")\n",
        "\n",
        "results['Logistic Regression (Optimized)'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_lr_best),\n",
        "    'precision': precision_score(y_test, y_pred_lr_best),\n",
        "    'recall': recall_score(y_test, y_pred_lr_best),\n",
        "    'f1': f1_score(y_test, y_pred_lr_best),\n",
        "    'roc_auc': roc_auc_score(y_test, y_proba_lr_best),\n",
        "    'predictions': y_pred_lr_best,\n",
        "    'probabilities': y_proba_lr_best\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 5.2 SUPPORT VECTOR MACHINE (SVM)\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n--- SUPPORT VECTOR MACHINE (Linear SVC) ---\")\n",
        "\n",
        "print(\"\\nTraining Linear SVM on TF-IDF features...\")\n",
        "\n",
        "svm_model = LinearSVC(\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# LinearSVC doesn't have predict_proba, use decision_function\n",
        "y_scores_svm = svm_model.decision_function(X_test_tfidf)\n",
        "\n",
        "# Evaluation\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "prec_svm = precision_score(y_test, y_pred_svm)\n",
        "rec_svm = recall_score(y_test, y_pred_svm)\n",
        "f1_svm = f1_score(y_test, y_pred_svm)\n",
        "roc_auc_svm = roc_auc_score(y_test, y_scores_svm)\n",
        "\n",
        "results['SVM'] = {\n",
        "    'accuracy': acc_svm,\n",
        "    'precision': prec_svm,\n",
        "    'recall': rec_svm,\n",
        "    'f1': f1_svm,\n",
        "    'roc_auc': roc_auc_svm,\n",
        "    'predictions': y_pred_svm,\n",
        "    'probabilities': y_scores_svm\n",
        "}\n",
        "\n",
        "print(f\"\\nSVM Results:\")\n",
        "print(f\"  Accuracy:  {acc_svm:.4f}\")\n",
        "print(f\"  Precision: {prec_svm:.4f}\")\n",
        "print(f\"  Recall:    {rec_svm:.4f}\")\n",
        "print(f\"  F1-Score:  {f1_svm:.4f}\")\n",
        "print(f\"  ROC-AUC:   {roc_auc_svm:.4f}\")\n",
        "\n",
        "# Grid Search for SVM\n",
        "print(\"\\nPerforming Grid Search for SVM...\")\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'loss': ['hinge', 'squared_hinge']\n",
        "}\n",
        "\n",
        "grid_svm = GridSearchCV(\n",
        "    LinearSVC(max_iter=1000, random_state=42),\n",
        "    param_grid_svm,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_svm.best_params_}\")\n",
        "print(f\"Best F1-score (CV): {grid_svm.best_score_:.4f}\")\n",
        "\n",
        "y_pred_svm_best = grid_svm.predict(X_test_tfidf)\n",
        "y_scores_svm_best = grid_svm.decision_function(X_test_tfidf)\n",
        "\n",
        "print(f\"\\nOptimized SVM Test F1-Score: {f1_score(y_test, y_pred_svm_best):.4f}\")\n",
        "\n",
        "results['SVM (Optimized)'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_svm_best),\n",
        "    'precision': precision_score(y_test, y_pred_svm_best),\n",
        "    'recall': recall_score(y_test, y_pred_svm_best),\n",
        "    'f1': f1_score(y_test, y_pred_svm_best),\n",
        "    'roc_auc': roc_auc_score(y_test, y_scores_svm_best),\n",
        "    'predictions': y_pred_svm_best,\n",
        "    'probabilities': y_scores_svm_best\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# 5.3 NAIVE BAYES (BONUS)\n",
        "# -----------------------------\n",
        "\n",
        "print(\"\\n--- NAIVE BAYES (Bonus Model) ---\")\n",
        "\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
        "y_proba_nb = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "results['Naive Bayes'] = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred_nb),\n",
        "    'precision': precision_score(y_test, y_pred_nb),\n",
        "    'recall': recall_score(y_test, y_pred_nb),\n",
        "    'f1': f1_score(y_test, y_pred_nb),\n",
        "    'roc_auc': roc_auc_score(y_test, y_proba_nb),\n",
        "    'predictions': y_pred_nb,\n",
        "    'probabilities': y_proba_nb\n",
        "}\n",
        "\n",
        "print(f\"\\nNaive Bayes Results:\")\n",
        "print(f\"  Accuracy:  {results['Naive Bayes']['accuracy']:.4f}\")\n",
        "print(f\"  F1-Score:  {results['Naive Bayes']['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC:   {results['Naive Bayes']['roc_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n Traditional ML models training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KglkSjpbKkKH"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 6. BERT MODEL (FINE-TUNING)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BERT MODEL - FINE-TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use smaller subset for BERT fine-tuning (resource-intensive)\n",
        "# For production, use full dataset\n",
        "BERT_SAMPLE_SIZE = 5000  # Adjust based on available resources\n",
        "\n",
        "print(f\"\\nUsing {BERT_SAMPLE_SIZE} samples for BERT fine-tuning...\")\n",
        "\n",
        "# Sample data\n",
        "X_train_bert = X_train.sample(n=min(BERT_SAMPLE_SIZE, len(X_train)), random_state=42)\n",
        "y_train_bert = y_train[X_train_bert.index]\n",
        "\n",
        "X_test_bert = X_test.sample(n=min(1000, len(X_test)), random_state=42)\n",
        "y_test_bert = y_test[X_test_bert.index]\n",
        "\n",
        "# Use original cleaned text (not processed) for BERT\n",
        "X_train_bert_text = df.loc[X_train_bert.index, 'review_cleaned'].tolist()\n",
        "X_test_bert_text = df.loc[X_test_bert.index, 'review_cleaned'].tolist()\n",
        "\n",
        "print(f\"BERT training samples: {len(X_train_bert_text)}\")\n",
        "print(f\"BERT test samples: {len(X_test_bert_text)}\")\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "print(\"\\nLoading BERT tokenizer and model...\")\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "print(\" BERT model loaded successfully!\")\n",
        "\n",
        "# Tokenize data\n",
        "print(\"\\nTokenizing data...\")\n",
        "\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=256,  # Limit sequence length\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    X_train_bert_text,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "test_encodings = tokenizer(\n",
        "    X_test_bert_text,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "# Create Dataset objects\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_encodings['input_ids'],\n",
        "    'attention_mask': train_encodings['attention_mask'],\n",
        "    'labels': y_train_bert.tolist()\n",
        "})\n",
        "\n",
        "test_dataset = Dataset.from_dict({\n",
        "    'input_ids': test_encodings['input_ids'],\n",
        "    'attention_mask': test_encodings['attention_mask'],\n",
        "    'labels': y_test_bert.tolist()\n",
        "})\n",
        "\n",
        "print(\" Tokenization complete!\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bert_results',\n",
        "    num_train_epochs=2,  # Increase for better performance\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    prec = precision_score(labels, predictions)\n",
        "    rec = recall_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_bert,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Fine-tune BERT\n",
        "print(\"\\nFine-tuning BERT model (this will take several minutes)...\")\n",
        "print(\"NOTE: For demo purposes, using 2 epochs. Increase for production.\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n BERT fine-tuning complete!\")\n",
        "\n",
        "# Evaluate BERT\n",
        "print(\"\\nEvaluating BERT model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nBERT Evaluation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    if not key.startswith('eval_'):\n",
        "        continue\n",
        "    metric_name = key.replace('eval_', '')\n",
        "    print(f\"  {metric_name.capitalize()}: {value:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred_bert = np.argmax(predictions.predictions, axis=-1)\n",
        "y_proba_bert = torch.softmax(torch.tensor(predictions.predictions), dim=-1)[:, 1].numpy()\n",
        "\n",
        "results['BERT'] = {\n",
        "    'accuracy': accuracy_score(y_test_bert, y_pred_bert),\n",
        "    'precision': precision_score(y_test_bert, y_pred_bert),\n",
        "    'recall': recall_score(y_test_bert, y_pred_bert),\n",
        "    'f1': f1_score(y_test_bert, y_pred_bert),\n",
        "    'roc_auc': roc_auc_score(y_test_bert, y_proba_bert),\n",
        "    'predictions': y_pred_bert,\n",
        "    'probabilities': y_proba_bert\n",
        "}\n",
        "\n",
        "print(\"\\n BERT evaluation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 7. MODEL COMPARISON & EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON & EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
        "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
        "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
        "    'F1-Score': [results[m]['f1'] for m in results.keys()],\n",
        "    'ROC-AUC': [results[m]['roc_auc'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(comparison_df)))\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "\n",
        "    axes[row, col].barh(comparison_df['Model'], comparison_df[metric], color=colors)\n",
        "    axes[row, col].set_xlabel(metric, fontsize=11, fontweight='bold')\n",
        "    axes[row, col].set_xlim(0, 1)\n",
        "    axes[row, col].set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[row, col].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(comparison_df[metric]):\n",
        "        axes[row, col].text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "# Remove empty subplot\n",
        "fig.delaxes(axes[1, 2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Overall performance radar chart\n",
        "from math import pi\n",
        "\n",
        "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "N = len(categories)\n",
        "\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "# Plot each model\n",
        "for idx, model in enumerate(comparison_df['Model'][:4]):  # Limit to 4 for clarity\n",
        "    values = comparison_df[comparison_df['Model']==model][categories].values.flatten().tolist()\n",
        "    values += values[:1]\n",
        "\n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=model)\n",
        "    ax.fill(angles, values, alpha=0.15)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories, size=11)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('Model Performance Radar Chart', size=16, fontweight='bold', pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k3NVZGzRXpUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 8. CONFUSION MATRICES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n--- CONFUSION MATRICES ---\")\n",
        "\n",
        "# Select top 3 models for detailed visualization\n",
        "top_models = ['Logistic Regression (Optimized)', 'SVM (Optimized)', 'BERT']\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, model_name in enumerate(top_models):\n",
        "    if model_name not in results:\n",
        "        continue\n",
        "\n",
        "    if model_name == 'BERT':\n",
        "        cm = confusion_matrix(y_test_bert, results[model_name]['predictions'])\n",
        "    else:\n",
        "        cm = confusion_matrix(y_test, results[model_name]['predictions'])\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification reports\n",
        "for model_name in top_models:\n",
        "    if model_name not in results:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Classification Report: {model_name}\")\n",
        "    print('='*60)\n",
        "\n",
        "    if model_name == 'BERT':\n",
        "        print(classification_report(y_test_bert, results[model_name]['predictions'],\n",
        "                                   target_names=['Negative', 'Positive']))\n",
        "    else:\n",
        "        print(classification_report(y_test, results[model_name]['predictions'],\n",
        "                                   target_names=['Negative', 'Positive']))\n"
      ],
      "metadata": {
        "id": "WpJ1IzRGXvd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 9. ROC CURVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n--- ROC CURVES ---\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for model_name in top_models:\n",
        "    if model_name not in results:\n",
        "        continue\n",
        "\n",
        "    if model_name == 'BERT':\n",
        "        fpr, tpr, _ = roc_curve(y_test_bert, results[model_name]['probabilities'])\n",
        "        roc_auc = results[model_name]['roc_auc']\n",
        "    else:\n",
        "        fpr, tpr, _ = roc_curve(y_test, results[model_name]['probabilities'])\n",
        "        roc_auc = results[model_name]['roc_auc']\n",
        "\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dy40aJhNX2v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 10. FEATURE IMPORTANCE (LOGISTIC REGRESSION)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n--- FEATURE IMPORTANCE ANALYSIS ---\")\n",
        "\n",
        "# Get coefficients from optimized Logistic Regression\n",
        "lr_best_model = grid_lr.best_estimator_\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "coefficients = lr_best_model.coef_[0]\n",
        "\n",
        "# Top positive features (indicate positive sentiment)\n",
        "top_positive_idx = np.argsort(coefficients)[-20:]\n",
        "top_positive_features = [(feature_names[i], coefficients[i]) for i in top_positive_idx]\n",
        "\n",
        "# Top negative features (indicate negative sentiment)\n",
        "top_negative_idx = np.argsort(coefficients)[:20]\n",
        "top_negative_features = [(feature_names[i], coefficients[i]) for i in top_negative_idx]\n",
        "\n",
        "print(\"\\nTop 20 Features Indicating POSITIVE Sentiment:\")\n",
        "for feature, coef in reversed(top_positive_features[:10]):\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "\n",
        "print(\"\\nTop 20 Features Indicating NEGATIVE Sentiment:\")\n",
        "for feature, coef in top_negative_features[:10]:\n",
        "    print(f\"  {feature}: {coef:.4f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Positive features\n",
        "pos_words, pos_coefs = zip(*reversed(top_positive_features))\n",
        "axes[0].barh(pos_words, pos_coefs, color='green', alpha=0.7)\n",
        "axes[0].set_xlabel('Coefficient Value', fontsize=11, fontweight='bold')\n",
        "axes[0].set_title('Top 20 Positive Sentiment Features', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Negative features\n",
        "neg_words, neg_coefs = zip(*top_negative_features)\n",
        "axes[1].barh(neg_words, neg_coefs, color='red', alpha=0.7)\n",
        "axes[1].set_xlabel('Coefficient Value', fontsize=11, fontweight='bold')\n",
        "axes[1].set_title('Top 20 Negative Sentiment Features', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XwNVMbgUYDGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 11. ERROR ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n--- ERROR ANALYSIS ---\")\n",
        "\n",
        "# Analyze misclassifications from best model (Optimized Logistic Regression)\n",
        "best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']\n",
        "print(f\"\\nAnalyzing errors from: {best_model_name}\")\n",
        "\n",
        "if 'BERT' not in best_model_name:\n",
        "    misclassified_idx = np.where(results[best_model_name]['predictions'] != y_test)[0]\n",
        "\n",
        "    print(f\"\\nTotal misclassifications: {len(misclassified_idx)} out of {len(y_test)} ({len(misclassified_idx)/len(y_test)*100:.2f}%)\")\n",
        "\n",
        "    # Show sample misclassifications\n",
        "    print(\"\\nSample Misclassified Reviews:\")\n",
        "    for i in misclassified_idx[:5]:\n",
        "        actual = y_test.iloc[i]\n",
        "        predicted = results[best_model_name]['predictions'][i]\n",
        "        review_text = X_test.iloc[i][:150]\n",
        "\n",
        "        print(f\"\\n{'-'*60}\")\n",
        "        print(f\"Review: {review_text}...\")\n",
        "        print(f\"Actual: {'Positive' if actual==1 else 'Negative'}\")\n",
        "        print(f\"Predicted: {'Positive' if predicted==1 else 'Negative'}\")\n"
      ],
      "metadata": {
        "id": "qeLpEQsaYOd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 12. PREDICTION EXAMPLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREDICTION EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test with custom reviews\n",
        "test_reviews = [\n",
        "    \"This movie was absolutely fantastic! The acting was superb and the plot was engaging.\",\n",
        "    \"Terrible film. Waste of time and money. Would not recommend to anyone.\",\n",
        "    \"It was okay, nothing special but not terrible either.\",\n",
        "    \"Best movie I've seen this year! Highly recommended!\",\n",
        "    \"Boring and predictable. Very disappointing.\"\n",
        "]\n",
        "\n",
        "print(\"\\nMaking predictions on custom reviews using Optimized Logistic Regression...\\n\")\n",
        "\n",
        "for review in test_reviews:\n",
        "    # Preprocess\n",
        "    cleaned = clean_text(review)\n",
        "    processed = preprocess_text(cleaned)\n",
        "\n",
        "    # Vectorize\n",
        "    vectorized = tfidf_vectorizer.transform([processed])\n",
        "\n",
        "    # Predict\n",
        "    prediction = grid_lr.predict(vectorized)[0]\n",
        "    probability = grid_lr.predict_proba(vectorized)[0]\n",
        "\n",
        "    sentiment = \"POSITIVE\" if prediction == 1 else \"NEGATIVE\"\n",
        "    confidence = probability[prediction] * 100\n",
        "\n",
        "    print(f\"Review: \\\"{review}\\\"\")\n",
        "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.1f}%)\")\n",
        "    print(f\"Probabilities: Negative={probability[0]:.3f}, Positive={probability[1]:.3f}\\n\")"
      ],
      "metadata": {
        "id": "05oyzAgTYfLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZyoXbGY-MDx"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 13. FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS COMPLETE - FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n DATASET STATISTICS\")\n",
        "print(f\"{''*60}\")\n",
        "print(f\"  * Total Reviews: {len(df):,}\")\n",
        "print(f\"  * Training Samples: {len(X_train):,}\")\n",
        "print(f\"  * Test Samples: {len(X_test):,}\")\n",
        "print(f\"  * Average Review Length: {df['word_count'].mean():.1f} words\")\n",
        "print(f\"  * Sentiment Balance: 50/50 (Balanced)\")\n",
        "\n",
        "print(f\"\\n FEATURE ENGINEERING\")\n",
        "print(f\"{''*60}\")\n",
        "print(f\"  * Bag of Words Vocabulary: {len(bow_vectorizer.vocabulary_):,} terms\")\n",
        "print(f\"  * TF-IDF Vocabulary: {len(tfidf_vectorizer.vocabulary_):,} terms\")\n",
        "print(f\"  * Word2Vec Vocabulary: {len(w2v_model.wv):,} words\")\n",
        "print(f\"  * Word2Vec Embedding Dimension: {w2v_model.vector_size}\")\n",
        "\n",
        "print(f\"\\n MODEL PERFORMANCE\")\n",
        "print(f\"{''*60}\")\n",
        "\n",
        "# Best model\n",
        "best_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model = comparison_df.loc[best_idx, 'Model']\n",
        "best_f1 = comparison_df.loc[best_idx, 'F1-Score']\n",
        "best_accuracy = comparison_df.loc[best_idx, 'Accuracy']\n",
        "best_roc_auc = comparison_df.loc[best_idx, 'ROC-AUC']\n",
        "\n",
        "print(f\"  * Best Model: {best_model}\")\n",
        "print(f\"  * Best F1-Score: {best_f1:.4f}\")\n",
        "print(f\"  * Best Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"  * Best ROC-AUC: {best_roc_auc:.4f}\")\n",
        "\n",
        "print(f\"\\n MODEL RANKINGS (by F1-Score)\")\n",
        "print(f\"{''*60}\")\n",
        "ranked = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "for idx, row in ranked.iterrows():\n",
        "    print(f\"  {idx+1}. {row['Model']}: F1={row['F1-Score']:.4f}, Accuracy={row['Accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\n All analyses completed successfully!\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "# Save results\n",
        "print(\"Saving results...\")\n",
        "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
        "print(\"Results saved to 'model_comparison_results.csv'\")\n",
        "\n",
        "print(\"\\n Sentiment Analysis Pipeline Complete!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Path to your notebook\n",
        "nb_path = \"/content/Sedrick_assignment10_IAI.ipynb\"\n",
        "\n",
        "# Load the notebook\n",
        "with open(nb_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Remove broken widget metadata\n",
        "if 'widgets' in nb['metadata']:\n",
        "    nb['metadata'].pop('widgets')\n",
        "\n",
        "# Save cleaned notebook\n",
        "clean_path = nb_path.replace(\".ipynb\", \"_clean.ipynb\")\n",
        "with open(clean_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(\" Cleaned notebook saved as:\", clean_path)"
      ],
      "metadata": {
        "id": "hqgouZe3Ma3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install texlive-xetex texlive-fonts-recommended texlive-plain-generic -y"
      ],
      "metadata": {
        "id": "P7X9jGLNNB70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to pdf /content/Sedrick_assignment10_IAI_clean.ipynb"
      ],
      "metadata": {
        "id": "BKzEt2oTMqBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "RJ50UT34-1v_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMkx9kGe/PVOodjKiDOOd5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}